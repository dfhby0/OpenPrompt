{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\".\")\n",
    "\n",
    "\n",
    "from openprompt.trainer import ClassificationRunner, GenerationRunner\n",
    "from typing import Union\n",
    "from torch.nn.parallel.data_parallel import DataParallel\n",
    "from re import template\n",
    "from torch._C import device\n",
    "from openprompt.pipeline_base import PromptForClassification, PromptForGeneration\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "import torch\n",
    "from openprompt.utils.reproduciblity import set_seed\n",
    "from openprompt.plms import get_model_class\n",
    "from openprompt import PromptDataLoader, PromptModel\n",
    "from openprompt.prompts import load_template, load_verbalizer\n",
    "from openprompt.data_utils import FewShotSampler\n",
    "from openprompt.utils.logging import config_experiment_dir, init_logger, logger\n",
    "from openprompt.utils.metrics import classification_metrics\n",
    "from openprompt.utils.calibrate import calibrate\n",
    "from transformers import  AdamW, get_constant_schedule_with_warmup, get_linear_schedule_with_warmup\n",
    "from openprompt.config import get_yaml_config\n",
    "from openprompt.plms import load_plm\n",
    "from openprompt.data_utils import load_dataset\n",
    "from openprompt.utils.cuda import model_to_device\n",
    "from openprompt.utils.utils import check_config_conflicts\n",
    "import logging\n",
    "\n",
    "def get_config(config_yaml = ''):\n",
    "    if config_yaml == '':\n",
    "        parser = argparse.ArgumentParser(\"classification config\")\n",
    "        parser.add_argument(\"--config_yaml\", type=str, help='the configuration file for this experiment.')\n",
    "        parser.add_argument(\"--resume\", action=\"store_true\", help='whether to resume a training from the latest checkpoint.\\\n",
    "            It will fall back to run from initialization if no lastest checkpoint are found.')\n",
    "        parser.add_argument(\"--test\", action=\"store_true\", help='whether to resume a training from the latest checkpoint.\\\n",
    "            It will fall back to run from initialization if no lastest checkpoint are found.') #\n",
    "        args = parser.parse_args()\n",
    "        config = get_yaml_config(args.config_yaml)\n",
    "\n",
    "    else:\n",
    "        config = get_yaml_config(config_yaml)\n",
    "        args = ''\n",
    "    check_config_conflicts(config)\n",
    "    logger.info(\"CONFIGS:\\n{}\\n{}\\n\".format(config, \"=\"*40))\n",
    "    return config, args\n",
    "\n",
    "\n",
    "def build_dataloader(dataset, template, tokenizer, config, split):\n",
    "    dataloader = PromptDataLoader(dataset=dataset, \n",
    "                                template=template, \n",
    "                                tokenizer=tokenizer, \n",
    "                                batch_size=config[split].batch_size,\n",
    "                                shuffle=config[split].shuffle_data,\n",
    "                                teacher_forcing=config[split].teacher_forcing \\\n",
    "                                    if hasattr(config[split],'teacher_forcing') else None,\n",
    "                                predict_eos_token=True if config.task==\"generation\" else False,\n",
    "                                **config.dataloader\n",
    "                                )\n",
    "    return dataloader\n",
    "\n",
    "def save_config_to_yaml(config):\n",
    "    from contextlib import redirect_stdout\n",
    "    saved_yaml_path = os.path.join(config.logging.path, \"config.yaml\")\n",
    "    with open(saved_yaml_path, 'w') as f:\n",
    "        with redirect_stdout(f): print(config.dump())\n",
    "    logger.info(\"Config saved as {}\".format(saved_yaml_path))\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "config = get_config(config_yaml='experiments/relation_classification_ptr.')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "EXP_PATH = config_experiment_dir(config)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "\n",
    "# init logger, create log dir and set log level, etc.\n",
    "EXP_PATH = config_experiment_dir(config)\n",
    "\n",
    "init_logger(EXP_PATH+\"/log.txt\", config.logging.file_level, config.logging.console_level)\n",
    "# save config to the logger directory\n",
    "\n",
    "set_seed(config)\n",
    "# load the pretrained models, its model, tokenizer, and config.\n",
    "plm_model, plm_tokenizer, plm_config = load_plm(config)\n",
    "# load dataset. The valid_dataset can be None\n",
    "train_dataset, valid_dataset, test_dataset, Processor = load_dataset(config)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NotADirectoryError",
     "evalue": "logging base directory `./logs` not found",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotADirectoryError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3117732/2757075199.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# init logger, create log dir and set log level, etc.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mEXP_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig_experiment_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0minit_logger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEXP_PATH\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/log.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_level\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconsole_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# save config to the logger directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/zm/OpenPrompt/openprompt/utils/logging.py\u001b[0m in \u001b[0;36mconfig_experiment_dir\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \"\"\"\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath_base\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotADirectoryError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"logging base directory `{config.logging.path_base}` not found\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# generate unique string\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotADirectoryError\u001b[0m: logging base directory `./logs` not found"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if config.task == \"classification\":\n",
    "    # define prompt\n",
    "    template = load_template(config=config, model=plm_model, tokenizer=plm_tokenizer, plm_config=plm_config)\n",
    "    verbalizer = load_verbalizer(config=config, model=plm_model, tokenizer=plm_tokenizer, plm_config=plm_config, classes=Processor.labels)\n",
    "    # load promptâ€™s pipeline model\n",
    "    prompt_model = PromptForClassification(plm_model, template, verbalizer)\n",
    "elif config.task == \"generation\":\n",
    "    template = load_template(config=config, model=plm_model, tokenizer=plm_tokenizer, plm_config=plm_config)\n",
    "    prompt_model = PromptForGeneration(plm_model, template, gen_config=config.generation)\n",
    "# move the model to device:\n",
    "prompt_model = model_to_device(prompt_model, config.environment)\n",
    "\n",
    "# process data and get data_loader\n",
    "if config.learning_setting == 'full':\n",
    "    pass\n",
    "elif config.learning_setting == 'few_shot':\n",
    "    if config.few_shot.few_shot_sampling is not None:\n",
    "        sampler = FewShotSampler(\n",
    "            num_examples_per_label = config.sampling_from_train.num_examples_per_label,\n",
    "            also_sample_dev = config.sampling_from_train.also_sample_dev,\n",
    "            num_examples_per_label_dev = config.sampling_from_train.num_examples_per_label_dev\n",
    "        )\n",
    "        train_dataset, valid_dataset = sampler(\n",
    "            train_dataset = train_dataset,\n",
    "            valid_dataset = valid_dataset,\n",
    "            seed = config.sampling_from_train.seed\n",
    "        )\n",
    "elif config.learning_setting == 'zero_shot':\n",
    "    pass\n",
    "\n",
    "if config.calibrate is not None:\n",
    "    assert isinstance(prompt_model, PromptForClassification), \"The type of model doesn't support calibration.\"\n",
    "    calibrate(prompt_model, config)\n",
    "\n",
    "train_dataloader = build_dataloader(train_dataset, template, plm_tokenizer, config, \"train\")\n",
    "valid_dataloader = build_dataloader(valid_dataset, template, plm_tokenizer, config, \"dev\")\n",
    "test_dataloader = build_dataloader(test_dataset, template, plm_tokenizer, config, \"test\")\n",
    "print(train_dataloader)\n",
    "\n",
    "# test_dataloader = valid_dataloader  # if the test size is big, replace it with valid_dataloader for debugging.\n",
    "if config.task == \"classification\":\n",
    "    runner = ClassificationRunner(prompt_model = prompt_model,\n",
    "                            train_dataloader = train_dataloader,\n",
    "                            valid_dataloader = valid_dataloader,\n",
    "                            test_dataloader = test_dataloader,\n",
    "                            config = config)\n",
    "elif config.task == \"generation\":\n",
    "    runner = GenerationRunner(prompt_model = prompt_model,\n",
    "                            train_dataloader = train_dataloader,\n",
    "                            valid_dataloader = valid_dataloader,\n",
    "                            test_dataloader = test_dataloader,\n",
    "                            config = config)\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "runner.run()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "runner.test()#"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "runner.resume()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('prompt': conda)"
  },
  "interpreter": {
   "hash": "b7ebb03698c1f30290bf119b7cfbb41ed204db7ee6e47857473ff9133b89e8db"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}